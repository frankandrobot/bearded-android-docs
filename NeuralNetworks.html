<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title></title>
		<meta name='Generator' content='Zim 0.60'>
		<style type='text/css'>
			a          { text-decoration: none      }
			a:hover    { text-decoration: underline }
			a:active   { text-decoration: underline }
			strike     { color: grey                }
			u          { text-decoration: none;
			             background-color: yellow   }
			tt         { color: #2e3436;            }
			pre        { color: #2e3436;
			             margin-left: 20px          }
			h1         { text-decoration: underline;
			             color: #4e9a06             }
			h2         { color: #4e9a06             }
			h3         { color: #4e9a06             }
			h4         { color: #4e9a06             }
			h5         { color: #4e9a06             }
			span.insen { color: grey                }
		</style>

<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/2.3-latest/MathJax.js?config=AM_HTMLorMML.js">
</script>

	</head>
	<body>

<!-- Header -->

	[ <a href='./MultiThreadingTechniques/Volatile.html'>Prev</a> ]

			[ <a href='./index.html.html'>Index</a> ]

	[ <a href='./NeuralNetworks/Appendix.html'>Next</a> ]

<!-- End Header -->

<hr />

<!-- Wiki content -->

<h1>NeuralNetworks</h1>

<p>
Created Friday 15 November 2013<br>
</p>

<h2>Definition of a Neuron</h2>
<p>
A <strong>neuron</strong> N is a pair `(bb w, phi)`, where `bb w` is the bias plus a list of `k` weights `(w_0, w_1,... , w_k)` and `phi` is an <a href="./NeuralNetworks/ActivationFunction.html" title="activation function" class="page">activation function</a>. We call `w_0` the bias weight. (Note: alternatively, we can put the bias at the end of the list. We would just need to redefine `x_0` accordingly. See below.)<br>
</p>

<p>
If `v_N: RR^(k+1) -&gt; RR` is defined as<br>
</p>

<p>
`\ v_N(bb x) = bb w * bb x = bb sum_i^k w_i x_i`<br>
</p>

<p>
then the value of `v_N` at a specific `bb x` is called the <strong>induced local field </strong>of neuron N.<br>
</p>

<p>
The <strong>impulse function</strong> of a neuron N, which we will denote `f_N: RR^(k+1) -&gt; (0,1)`, is defined as<br>
</p>

<p>
`f_N(bb x) = phi(v_N(bb x)) = phi(bb w * bb x)`<br>
</p>

<p>
By convention, `x_0` (which is mapped to the bias) is fixed at 1 for all inputs `bb x`.<br>
</p>

<h3>Purpose of the Bias</h3>
<p>
A single neuron basically separates data with a line (or in general, a hyper plane). The bias allows for lines that do not pass through the origin. <br>
</p>

<h2>Definition of a Neural Network</h2>
<p>
A <strong>neural network </strong>can be thought of as a function that is itself the composition, sum, and product of impulse functions. See <a href="http://en.wikibooks.org/wiki/Artificial_Neural_Networks/Neural_Network_Basics" title="here" class="http">here</a> for diagrams or take a look at the <a href="./NeuralNetworks/Resources.html" title="resources." class="page">resources.</a> More importantly, since `phi` is continuous and differentiable, the function representing the neural network is itself continuous and differentiable. <br>
</p>
 



<!-- End wiki content -->

<hr />

<!-- Attachments and Backlinks -->

	<b>Backlinks:</b>		<a href='./NeuralNetworks/BackPropagation.html'>NeuralNetworks:BackPropagation</a>
<br><br>

	<b>Attachments:</b>
	<table>		<tr><td><a href='./NeuralNetworks/ActivationFunction'>ActivationFunction</a></td><td>&nbsp;</td><td>4.10kb</td></tr>		<tr><td><a href='./NeuralNetworks/Appendix'>Appendix</a></td><td>&nbsp;</td><td>4.10kb</td></tr>		<tr><td><a href='./NeuralNetworks/BackPropagation'>BackPropagation</a></td><td>&nbsp;</td><td>4.10kb</td></tr>	</table>

<!-- End Attachments and Backlinks -->

	</body>

</html>
