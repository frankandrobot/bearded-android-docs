<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title></title>
		<meta name='Generator' content='Zim 0.60'>
		<style type='text/css'>
			a          { text-decoration: none      }
			a:hover    { text-decoration: underline }
			a:active   { text-decoration: underline }
			strike     { color: grey                }
			u          { text-decoration: none;
			             background-color: yellow   }
			tt         { color: #2e3436;            }
			pre        { color: #2e3436;
			             margin-left: 20px          }
			h1         { text-decoration: underline;
			             color: #4e9a06             }
			h2         { color: #4e9a06             }
			h3         { color: #4e9a06             }
			h4         { color: #4e9a06             }
			h5         { color: #4e9a06             }
			span.insen { color: grey                }
		</style>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=AM_HTMLorMML.js">
</script>

	</head>
	<body>

<!-- Header -->

	[ <a href='./MultiThreadingTechniques/Volatile.html'>Prev</a> ]

			[ <a href='./index.html.html'>Index</a> ]

	[ <a href='./NeuralNetworks/Appendix.html'>Next</a> ]

<!-- End Header -->

<hr />

<!-- Wiki content -->

<h1>NeuralNetworks</h1>

<p>
Created Friday 15 November 2013<br>
</p>

<p>
<ul>
<li><a href="http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/" title="http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/" class="http">http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/</a></li>
</ul>
</p>
 
<p>
A <strong>neuron</strong> N is a pair `(W, sigma)`, where W is the bias plus a list of `k` weights `(w_0, w_1,... , w_k)`. We call `w_0` the bias weight. (Note: implementations may put the bias at the end of the list.)<br>
</p>

<p>
If `v_N: RR^(k+1) -&gt; RR` is defined as<br>
</p>

<p>
`v_N(bb x) = sum_i^k w_i x_i`<br>
</p>

<p>
Then the value of `v_N` at a specific `bb x` is called the <strong>induced local field </strong>of neuron N.<br>
</p>

<p>
If `sigma` is an <a href="./NeuralNetworks/ActivationFunction.html" title="+ActivationFunction" class="page">+ActivationFunction</a> , then the <strong>impulse function</strong> of a neuron N, which we will denote `f_N: RR^(k+1) -&gt; [0,1]`, is defined as<br>
</p>

<p>
`f_N(bb x) = sigma(v_N(bb x))`<br>
</p>

<p>
By convention, `x_0` (which is mapped to the bias) is fixed at 1 for all inputs `bb x`.<br>
</p>

<h2>Purpose of the Bias</h2>
<p>
In the Perceptron model we allowed a “bias” b which translated the separating hyperplane so that it need not pass through the origin, hence allowing a the set of all pairs `(bb w, b)` to represent every possible hyperplane.<br>
</p>



<!-- End wiki content -->

<hr />

<!-- Attachments and Backlinks -->

	<b>Backlinks:</b>		<a href='./NeuralNetworks/BackPropagation.html'>NeuralNetworks:BackPropagation</a>
<br><br>

	<b>Attachments:</b>
	<table>		<tr><td><a href='./NeuralNetworks/ActivationFunction'>ActivationFunction</a></td><td>&nbsp;</td><td>0b</td></tr>		<tr><td><a href='./NeuralNetworks/Appendix'>Appendix</a></td><td>&nbsp;</td><td>4.10kb</td></tr>		<tr><td><a href='./NeuralNetworks/BackPropagation'>BackPropagation</a></td><td>&nbsp;</td><td>4.10kb</td></tr>	</table>

<!-- End Attachments and Backlinks -->

	</body>

</html>
