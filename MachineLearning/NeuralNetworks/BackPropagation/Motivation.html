<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Bearded-android-docs : Android quick docs" />

    <link rel="stylesheet" type="text/css" media="screen" href="
http://frankandrobot.github.io/bearded-android-docs/stylesheets/stylesheet.css">

    <title>Motivation</title>
    <meta name='Generator' content='Zim 0.60'>
    
    <script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/2.3-latest/MathJax.js?config=AM_HTMLorMML.js">
</script>

  </head>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/frankandrobot/bearded-android-docs">View on GitHub</a>

          <h1 id="project_title">Bearded-android-docs</h1>
          <h2 id="project_tagline">Motivation</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/frankandrobot/bearded-android-docs/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/frankandrobot/bearded-android-docs/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

<!-- Wiki content -->

<!-- -->
<!-- <h1>Motivation</h1> -->
<!-- -->

<p>
Created Thursday 21 November 2013<br>
</p>

<p>
<ul>
<li><a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf" title="http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf" class="http">http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf</a></li>
<li><a href="http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/" title="http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/" class="http">http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/</a></li>
</ul>
</p>

<h2>Motivation for the Backpropagation Algorithm</h2>
<p>
<ul>
<li>Goal: for fixed training example `bb x`, we want the NN to output a specific output `bb t`. </li>
<li>We need to somehow modify the weights `bb w` so that the output is as close as possible to `bb t`.</li>
<li>We can measure the difference between the actual output and expected output and somehow minimize the error.</li>
<li>The total error measures the difference between the actual and expected outputs.</li>
<li>The total error is actually function of the weights! (Each training example is fixed. The weights are the only parameters that can minimize the error function.)</li>
<li>The total error is continuous and differentiable! (Each impulse function is continuous and differentiable. E is built via composition and summation of the impulse functions. Thus, its gradient exists.)</li>
<li>As a result, we can use the method of <em>gradient descent</em> to minimize the error function and find the values of the weights. In a nutshell, this is the backpropagation algorithm.</li>
</ul>
</p>

<h2>Derivation of the Backpropagation Algorithm (Sort of)</h2>

<h3>The Gradient of the total error `E`</h3>
<p>
Consider a single-layer network with only a single neuron `(bb w,phi)` and a single training example `(bb x, bb t)`. Since the NN has a single neuron, `bb t` is a one-dimensional vector i.e., `bb t=t`, a scalar.<br>
</p>

<p>
Then the total error is `E=E[1]`.<br>
</p>

<p>
But wait there's more! Since the NN has only a single neuron, the output is just the value of the neuron's impulse function `f`, so <br>
</p>

<p>
`\ E = 1/2 ||f(bb x) - t||^2 = 1/2 ||phi(bb w * bb x) - t||^2 = (phi(bb w * bb x) - t)^2`<br>
</p>

<p>
where E is a function of `bb w`.<br>
</p>

<p>
The gradient of E is<br>
</p>

<p>
`\ grad E = ((dE)/(dw_0),... ,(dE)/(dw_k))`.<br>
</p>

<p>
Let's compute a `(dE)/(dw_j)`.<br>
</p>

<p>
`\ (dE)/(dw_j) = (d)/(dw_j) [phi(sum_k w_k x_k) - t]^2`<br>
</p>

<p>
`t` and each `x_k` are constants. Also, each `w_i != w_j` is  constant. So the expression is really of the form<br>
</p>

<p>
`\ (dE)/(dw_j) = (d)/(dw_j) (phi(C + w_j x_j) - D)^2`<br>
</p>

<p>
where `C` and `D` are constants. Applying the chain rule twice (once on the squared term and then on `phi`)<br>
</p>

<p>
`\ \ \ (dE)/(dw_j)`<br>
`\ \ = 2(phi(C + w_j x_j) - D) xx (d)/(dw_j)(phi(C + w_j x_j) - D)`<br>
`\ \ = 2(phi(C + w_j x_j) - D) xx phi'(C + w_j x_j) xx x_j`<br>
`\ \ = 2[phi(sum_k w_k x_k) - t] xx phi'(sum_k w_k x_k) xx x_j`<br>
</p>

<p>
Phew! Let's make a simplification. `phi(sum_k w_k x_k)` is just the output `o` of the neuron, so<br>
</p>

<p>
`\ \ (dE)/(dw_j) = 2(o - t) phi'(o) x_j`<br>
</p>

<h3>The Backpropagation Algorithm</h3>
<p>
The gradient points in the direction of steepest ascent, so if we want to go to the minimum, just travel backwards along the gradient. In other words, add a negative multiple of `(dE)/(dw_j)` to the current weight `w_j` to go towards the minimum of `E`:<br>
</p>

<p>
`\ \ w_j = w_j - eta (o - t) phi'(o) x_j`<br>
</p>

<p>
(We drop the "2" since it gets absorbed by `eta`. `eta` is called the <strong>learning parameter</strong>.)<br>
</p>

<p>
Let's step back for a minute. We're adjusting the weights (which are the real inputs) so that the error function E is at its global minimum (hopefully). The problem is how can we be sure its a global minimum? how can we be sure the adjustment doesn't overshoot the minimum? how can we be sure we don't oscillate near the minimum?<br>
</p>

<p>
Enter the <strong>momentum</strong>.<br>
</p>

<p>
`\ \ w_j = w_j - eta (o - t) phi'(o) x_j + alpha w_j[prev]`<br>
</p>

<p>
where `alpha` is the <strong>momentum parameter</strong> and `w_j[prev]` is the value of `w_j` from the previous iteration of the backpropagation algorithm.<br>
</p>

<p>
<strong>The backpropagation algorithm repeats this process until the error E is smaller than a preset value.</strong><br>
</p>

<p>
More generally, we can apply the same reasoning to an NN with more than one neuron and more than one layer. In the general case, the formula for a gradient is a function of the gradients in the <em>next</em> layer. Hence, the name <em>backprogapation</em> since you must start in the output layer and work backwards to find the weights.<br>
</p>


<!-- End wiki content -->

<hr />

<!-- Attachments and Backlinks -->

<h3>Backlinks:</h3>	<a href='../BackPropagation.html'>MachineLearning:NeuralNetworks:BackPropagation</a>

<h3>Attachments:</h3>
<table>	<tr><td><a href='./Motivation/diagram.dot'>diagram.dot</a></td><td>&nbsp;</td><td>736b</td></tr>	<tr><td><a href='./Motivation/diagram.png'>diagram.png</a></td><td>&nbsp;</td><td>3.76kb</td></tr></table>

<!-- End Attachments and Backlinks -->

    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'beardedandroid'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Bearded-android-docs maintained by <a href="https://github.com/frankandrobot">frankandrobot</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
        <p><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>

      </footer>
    </div>

    

  </body>
</html>
