Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2014-07-27T21:32:45-04:00

====== LearningWithBackprop ======
Created Sunday 27 July 2014

The [[NeuralNetworks:BackPropagation|standard algorithm]] must be slighthly modified to take into account the weight sharing. An easy way is:

1. calculate the [[NeuralNetworks:BackPropagation:BackPropAlgorithms:BatchModeBackPropagation:UpdatingTheLearningTerms|cumulative learning term]] of each weight (gradient of each weight) ignoring the weight sharing i.e., pretend that the network is a conventional multi-layered NN. //At this step, do not update the weights just yet. //Normally, each weight has one cumulative learning term. But because of the weight sharing, each weight has more than one cumulative learning term.
2. take the sum of all the cumulative learning terms of each weight. Proceed as you normally would using the backprop i.e., use this meta-cumulative learning term to update the weight, then add the momentum term.
