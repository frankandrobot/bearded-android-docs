<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>Motivation</title>
		<meta name='Generator' content='Zim 0.60'>
		<style type='text/css'>
			a          { text-decoration: none      }
			a:hover    { text-decoration: underline }
			a:active   { text-decoration: underline }
			strike     { color: grey                }
			u          { text-decoration: none;
			             background-color: yellow   }
			tt         { color: #2e3436;            }
			pre        { color: #2e3436;
			             margin-left: 20px          }
			h1         { text-align: center;
			             color: #4e9a06             }
			h2         { color: #4e9a06             }
			h3         { color: #4e9a06             }
			h4         { color: #4e9a06             }
			h5         { color: #4e9a06             }
			span.insen { color: grey                }
			.page { max-width: 1000px;}
			.menu{
				float:left; width: 300px;
			}

			.content { padding-left: 320px;}
			.notebook{font-variant: small-caps;
					color:#4e9a06;
					padding: 0px 20px;}
			hr{clear:both;}
		</style>
		<script type="text/javascript"
		   src="http://cdn.mathjax.org/mathjax/2.3-latest/MathJax.js?config=AM_HTMLorMML.js">
		</script>


	</head>
	<body>
		<div class="page">
			<div class="heading">
									<h1>Motivation</h1>
			</div>
			<hr>
			<div class="menu">
				<ul>
<li><a href="../../ActiveMQ.html" title="ActiveMQ" class="page">ActiveMQ</a></li>
<li><a href="../../Android.html" title="Android" class="page">Android</a></li>
<li><a href="../../ContextFreeGrammar.html" title="ContextFreeGrammar" class="page">ContextFreeGrammar</a></li>
<li><a href="../../Database.html" title="Database" class="page">Database</a></li>
<li><a href="../../Help.html" title="Help" class="page">Help</a></li>
<li><a href="../../HttpProtocol.html" title="HttpProtocol" class="page">HttpProtocol</a></li>
<li><a href="../../Java.html" title="Java" class="page">Java</a></li>
<li><a href="../../JavaMemory.html" title="JavaMemory" class="page">JavaMemory</a></li>
<li><a href="../../JavaScript.html" title="JavaScript" class="page">JavaScript</a></li>
<li><a href="../../MultiThreading.html" title="MultiThreading" class="page">MultiThreading</a></li>
<li><a href="../../MultiThreadingTechniques.html" title="MultiThreadingTechniques" class="page">MultiThreadingTechniques</a></li>
<li><a href="../../NETFramework.html" title="NETFramework" class="page">NETFramework</a></li>
<li><a href="../../NeuralNetworks.html" title="NeuralNetworks" class="page">NeuralNetworks</a></li>
<ul>
<li><a href="../ActivationFunction.html" title="ActivationFunction" class="page">ActivationFunction</a></li>
<li><a href="../Appendix.html" title="Appendix" class="page">Appendix</a></li>
<li><a href="../BackPropagation.html" title="BackPropagation" class="page">BackPropagation</a></li>
<ul>
<li><a href="./ErrorFunction.html" title="ErrorFunction" class="page">ErrorFunction</a></li>
<li><a href="./TrainingExamples.html" title="TrainingExamples" class="page">TrainingExamples</a></li>
<li><a href="./BatchModeBackPropagation.html" title="BatchModeBackPropagation" class="page">BatchModeBackPropagation</a></li>
<li><strong class="activepage">Motivation</strong></li>
<li><a href="./Parameters.html" title="Parameters" class="page">Parameters</a></li>
</ul>
<li><a href="../ConvolutionalNetwork.html" title="ConvolutionalNetwork" class="page">ConvolutionalNetwork</a></li>
<li><a href="../History.html" title="History" class="page">History</a></li>
<li><a href="../Neuron.html" title="Neuron" class="page">Neuron</a></li>
<li><a href="../Resources.html" title="Resources" class="page">Resources</a></li>
</ul>
<li><a href="../../OOP.html" title="OOP" class="page">OOP</a></li>
<li><a href="../../TODO.html" title="TODO" class="page">TODO</a></li>
<li><a href="../../UnitTests.html" title="UnitTests" class="page">UnitTests</a></li>
</ul>

			</div>
			<div class="content">
			<!-- Wiki content -->
				<p>
Created Thursday 21 November 2013<br>
</p>

<p>
<ul>
<li><a href="http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf" title="http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf" class="http">http://page.mi.fu-berlin.de/rojas/neural/chapter/K7.pdf</a></li>
<li><a href="http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/" title="http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/" class="http">http://jeremykun.com/2012/12/09/neural-networks-and-backpropagation/</a></li>
</ul>
</p>

<h2>Motivation for the Backpropagation Algorithm</h2>
<p>
<ul>
<li>The <a href="./ErrorFunction.html" title="total error" class="page">total error</a> E is actually function of the weights! Each <a href="./TrainingExamples.html" title="training example" class="page">training example</a> is fixed. The weights are the only parameters that can minimize the <a href="./ErrorFunction.html" title="error function" class="page">error function</a>.</li>
<li>The total error E is continuous and differentiable. Each impulse function is continuous and differentiable. E is built via composition and summation of the impulse functions. Thus, its gradient exists. </li>
<li>As a result, we can use the method of <em>gradient descent</em> to find the values of the weights. This is essentially all that the backpropagation algorithm is.</li>
</ul>
</p>

<h3>The Gradient of `E`</h3>
<p>
Let's start with the case of a single-layer network with only a single neuron.<br>
Then `E_i` reduces to `(o_(i1) - t_(i1))^2`, or dropping the extra subscript for the only neuron, `E_i = (o_i - t_i)^2`. But wait there's more! Since its a single neuron, the output is just the value of the neuron's impulse function `f`, so <br>
</p>

<p>
`E_i = (f(bb x_i) - t_i)^2 = [phi(sum_k w_k x_(ik)) - t_i]^2`.<br>
</p>

<p>
Thus, `E = sum_i [phi(sum_k w_k x_(ik)) - t_i]^2`.<br>
</p>

<p>
The gradient of E is defined as<br>
</p>

<p>
`grad E = ((dE)/(dw_0),... ,(dE)/(dw_k))`.<br>
</p>

<p>
Let's compute a `(dE)/(dw_j)` nice and slow. Recall `E` is the sum of `E_i`s.<br>
</p>

<p>
`(dE_i)/(dw_j) = (d)/(dw_j) [phi(sum_k w_k x_(ik)) - t_i]^2`<br>
</p>

<p>
`t_i` and each `x_(ik)` are constants. Also, we treat each `w_i != w_j` as constants. So the expression can be treated like this:<br>
</p>

<p>
`(dE_i)/(dw_j) = (d)/(dw_j) (phi(C + w_j x_(ij)) - D)^2`<br>
</p>

<p>
where `C` and `D` are constants. Applying the chain rule twice (once on the squared term and then on `phi`)<br>
</p>

<p>
`(dE_i)/(dw_j) = 2(phi(C + w_j x_(ij)) - D) xx (d)/(dw_j)(phi(C + w_j x_(ij)) - D)`<br>
`              = 2(phi(C + w_j x_(ij)) - D) xx phi'(C + w_j x_(ij)) xx x_(ij)`<br>
`              = 2[phi(sum_k w_k x_(ik)] - t_i) xx phi'(sum_k w_k x_(ik)) xx x_(ij)`<br>
</p>

<p>
Phew! Let's make a simplification. Recall the definition of the output `o_i`:<br>
</p>

<p>
`(dE_i)/(dw_j) = 2(o_i - t_i) phi'(o_i) x_(ij)`<br>
</p>

<p>
Thus:<br>
</p>

<p>
`(dE)/(dw_j) = 2 sum_(i=1)^p (o_i - t_i) phi'(o_i) x_(ij)`<br>
</p>

<p>
The gradient points in the direction of steepest ascent, so if we want to go to the minimum, just travel backwards along the gradient. In other words, add a negative multiple of `(dE)/(dw_j)` to the current weight `w_j` to go towards the minimum of `E`:<br>
</p>

<p>
`w_j = w_j - eta sum_(i=1)^p (o_i - t_i) phi'(o_i) x_(ij)`<br>
</p>

<p>
(We drop the "2" since it gets absorbed by `eta`. `eta` is called the <strong>learning parameter</strong>.)<br>
</p>

<p>
<ul>
<li><strong>The backpropagation algorithm repeats this process until the error E is smaller than a preset value.</strong></li>
</ul>
</p>

<p>
Let's step back for a minute. We're adjusting the weights (which are the real inputs) so that the error function E is at its global minimum (hopefully). The problem is how can we be sure its a global minimum? how can we be sure the adjustment doesn't overshoot the minimum? how can we be sure we don't oscillate near the minimum?<br>
</p>

<p>
Enter the <strong>momentum</strong>. <br>
</p>

<h3>More Complex Case</h3>
<p>
We still haven't seen why the backpropagation algorithm is named that way. Consider a slightly more complex network consisting of two layers and a single neuron in each layer. <br>
</p>

<p>
<img src="./Motivation/diagram.png" alt=""><br>
</p>

<p>
The output of the network is no longer a simple application of the impulse function `f_2` for the neuron in the output layer. <br>
Let `bb y = (y_1,y_2)` be the output of the hidden layer (the layer containing neuron `N_1`).<br>
Then the output of the network is<br>
</p>

<p>
`f_2(bb y) = phi(sum_k w_(k2)y_k)`<br>
</p>

<p>
The good news is that for `w_(j2)` (the weights of the output layer), `(dE_i)/(dw_(j2))` reduces to the previous case.<br>
</p>

<p>
`(dE_i)/(dw_(j2)) = 2(phi(C + w_(j2) y_j) - D) xx (d)/(dw_(j2))(phi(C + w_(j2) y_j) - D)`<br>
`                 = 2(phi(C + w_(j2) y_j) - D) xx phi'(C + w_(j2) y_j) xx y_j`<br>
`                 = 2[phi(sum_k w_(k2) y_k)] - t_i) xx phi'(sum_k w_(k2) y_k) xx y_j` <br>
</p>

<p>
What about for the weights in the hidden layer?<br>
</p>

<p>
`(dE_i)/(dw_(j1)) = (d)/(dw_(j1)) phi(sum_k w_(k2) y_k) - t_i]^2`<br>
`                 = (d)/(dw_(j1)) phi(sum_k w_(k2) ((sum phi(sum_l w_(l1) x_(il))) - t_i]^2`<br>
</p>

<p>
Continued...<br>
</p>


			<!-- End wiki content -->
			</div>
			<hr>
			<!-- Backlinks -->
			<div class="footer">
									<b>Backlinks:</b>						<a href='../BackPropagation.html'>NeuralNetworks:BackPropagation</a></li>
				<br><br>

									<b>Attachments:</b>
					<table>						<tr><td><a href='./Motivation/diagram.dot'>diagram.dot</a></td><td>&nbsp;</td><td>736b</td></tr>						<tr><td><a href='./Motivation/diagram.png'>diagram.png</a></td><td>&nbsp;</td><td>3.76kb</td></tr>					</table>
			<!-- End Backlinks -->
			</div>
		</div>
	</body>
</html>
