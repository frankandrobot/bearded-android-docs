<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3.org/TR/html4/loose.dtd">
<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
		<title>BatchModeBackPropagation</title>
		<meta name='Generator' content='Zim 0.60'>
		<style type='text/css'>
			a          { text-decoration: none      }
			a:hover    { text-decoration: underline }
			a:active   { text-decoration: underline }
			strike     { color: grey                }
			u          { text-decoration: none;
			             background-color: yellow   }
			tt         { color: #2e3436;            }
			pre        { color: #2e3436;
			             margin-left: 20px          }
			h1         { text-align: center;
			             color: #4e9a06             }
			h2         { color: #4e9a06             }
			h3         { color: #4e9a06             }
			h4         { color: #4e9a06             }
			h5         { color: #4e9a06             }
			span.insen { color: grey                }
			.page { max-width: 1000px;}
			.menu{
				float:left; width: 300px;
			}

			.content { padding-left: 320px;}
			.notebook{font-variant: small-caps;
					color:#4e9a06;
					padding: 0px 20px;}
			hr{clear:both;}
		</style>
		<script type="text/javascript"
		   src="http://cdn.mathjax.org/mathjax/2.3-latest/MathJax.js?config=AM_HTMLorMML.js">
		</script>


	</head>
	<body>
		<div class="page">
			<div class="heading">
									<h1>BatchModeBackPropagation</h1>
			</div>
			<hr>
			<div class="menu">
				<ul>
<li><a href="../../ActiveMQ.html" title="ActiveMQ" class="page">ActiveMQ</a></li>
<li><a href="../../Android.html" title="Android" class="page">Android</a></li>
<li><a href="../../ContextFreeGrammar.html" title="ContextFreeGrammar" class="page">ContextFreeGrammar</a></li>
<li><a href="../../Database.html" title="Database" class="page">Database</a></li>
<li><a href="../../Help.html" title="Help" class="page">Help</a></li>
<li><a href="../../HttpProtocol.html" title="HttpProtocol" class="page">HttpProtocol</a></li>
<li><a href="../../Java.html" title="Java" class="page">Java</a></li>
<li><a href="../../JavaMemory.html" title="JavaMemory" class="page">JavaMemory</a></li>
<li><a href="../../JavaScript.html" title="JavaScript" class="page">JavaScript</a></li>
<li><a href="../../MultiThreading.html" title="MultiThreading" class="page">MultiThreading</a></li>
<li><a href="../../MultiThreadingTechniques.html" title="MultiThreadingTechniques" class="page">MultiThreadingTechniques</a></li>
<li><a href="../../NETFramework.html" title="NETFramework" class="page">NETFramework</a></li>
<li><a href="../../NeuralNetworks.html" title="NeuralNetworks" class="page">NeuralNetworks</a></li>
<ul>
<li><a href="../ActivationFunction.html" title="ActivationFunction" class="page">ActivationFunction</a></li>
<li><a href="../Appendix.html" title="Appendix" class="page">Appendix</a></li>
<li><a href="../BackPropagation.html" title="BackPropagation" class="page">BackPropagation</a></li>
<ul>
<li><a href="./ErrorFunction.html" title="ErrorFunction" class="page">ErrorFunction</a></li>
<li><a href="./TrainingExamples.html" title="TrainingExamples" class="page">TrainingExamples</a></li>
<li><strong class="activepage">BatchModeBackPropagation</strong></li>
<li><a href="./Motivation.html" title="Motivation" class="page">Motivation</a></li>
<li><a href="./Parameters.html" title="Parameters" class="page">Parameters</a></li>
</ul>
<li><a href="../ConvolutionalNetwork.html" title="ConvolutionalNetwork" class="page">ConvolutionalNetwork</a></li>
<li><a href="../History.html" title="History" class="page">History</a></li>
<li><a href="../Neuron.html" title="Neuron" class="page">Neuron</a></li>
<li><a href="../Resources.html" title="Resources" class="page">Resources</a></li>
</ul>
<li><a href="../../OOP.html" title="OOP" class="page">OOP</a></li>
<li><a href="../../TODO.html" title="TODO" class="page">TODO</a></li>
<li><a href="../../UnitTests.html" title="UnitTests" class="page">UnitTests</a></li>
</ul>

			</div>
			<div class="content">
			<!-- Wiki content -->
				<p>
Created Thursday 12 December 2013<br>
</p>

<p>
Step 0. Pick values for the learning and momentum parameters. See <a href="./Parameters.html" title="backpropagation parameters" class="page">backpropagation parameters</a>. <br>
</p>

<p>
Step 1. Initialize the weights.<br>
</p>

<p>
Step 2. Go through each training example:<br>
<ul>
<li>2a. Perform the <em>forward propagation</em>. Starting with the input layer, store each <a href="../Neuron.html" title="neuron's" class="page">neuron's</a> induced local field and impulse function value.</li>
<li>2b. Perform the <em>backpropagation</em>. Starting with the output layer, calculate and store the gradients of each neuron.</li>
<li>2c. Update the cumulative learning term for each weight of each neuron starting with the output layer.</li>
<li>2d. Discard the stored induced local fields, impulse functions, and gradients.</li>
</ul>
</p>

<p>
Step 3. Adjust each weight using its cumulative learning term and momentum term. Make sure to store the values of the previous weights.<br>
Step 4. Construct the error function from the training examples.<br>
</p>

<p>
Step 5. Repeat steps 2--4 until the error is smaller than a pre-selected value. If the error never converges, pick new backpropagation parameters and restart from Step 0.<br>
</p>

<p>
<strong>Notes:</strong> <br>
<ol type="1" start="1">
<li>Its called the <em>backpropagation </em>algorithm because you start from the output layer (Steps 2b and c) and work backwards to the input layer.</li>
<li>Steps 2--4 are one iteration of the backpropagation algorithm.</li>
<li>A common mistake is adjust the weights before cycling through all the examples. You need to calculate a weight's <em>cumulative </em>learning term (from all the examples) before updating it.</li>
</ol>
</p>

<h2>Required Memory and Data Structures</h2>
<p>
You'll need data structure(s) to store:<br>
</p>

<p>
<ul>
<li>induced local fields and the values of the impulse functions of its neurons <em>for each layer</em>. Since these are just used to find the gradients and the learning terms, you do not need to store this data for each example. The data structure(s) can be shared by each example.</li>
<li>gradients of the neurons of each layer. <em>Ditto.</em> Since these are just used to find the learning terms, you do not need to store this data for each example. The data structure(s) can be shared by each example.</li>
<li>learning terms for the weights of the neurons in each layer. These represent the <em>cumulative </em>learning terms of all examples, so all you need is one set of data structure(s) per iteration.</li>
<li><em>previous </em>weight values (these are used by the momentum term). You do not need to store the complete history of the weights, just the previous values.</li>
</ul>
</p>

<h2>Initalizing the Weights</h2>
<p>
"Pick the weights from a uniform distribution whose mean is zero and whose variance is chosen to make the standard deviation of the induced local fields of the neurons (See <a href="../../NeuralNetworks.html" title="neural networks" class="page">neural networks</a>) lie at <em>the transition between the linear and saturated parts of the </em><a href="../ActivationFunction.html" title="activation function" class="page">activation function</a>" (I have no idea what the emphasized parts means.)<br>
</p>

<p>
Most researchers initialize the weights to random values between 0 and 1. I use a gaussian with mean 0.5 and standard deviation 1.<br>
</p>

<h2>Performing the Forward Propagation</h2>
<p>
<ul>
<li>Given a training input `bb x_i`, calculate the output of the neural network. </li>
<li>Make sure to store the induced local fields and the values of the impulse functions of each neuron in each layer.</li>
</ul>
</p>

<h2>Constructing the Gradients (`delta"s"`)</h2>
<p>
This is how to compute the local gradients (`delta"s"`) of each neuron:<br>
</p>

<p>
<ul>
<li>Let `N_j` be neuron `j`.</li>
<li>Let `L` be the output layer. </li>
<li>Let `v_j^((l))` be the induced local field of `N_j` in layer `l`. </li>
<li>Let `o_j^((l))` be the output (value of the impulse function) of `N_j` in layer `l`.</li>
<li>Let `gamma_j^((l))` be the derivative of `N_j`s <a href="../ActivationFunction.html" title="activation function" class="page">activation function</a>.</li>
<li>Let weight `w_(kj)^((l))` connect `N_k` in layer `l` to `N_j` in the previous layer.</li>
<li>(In general, let a superscript `l` denote layer `l`.)</li>
</ul>
</p>

<p>
<strong>Note: </strong>`gamma_k` is a function. `v_j` and `o_j` are real numbers.<br>
</p>

<p>
Define the local gradient `delta_j^((l))` of neuron `j` recursively as follows:<br>
</p>

<p>
<ul>
<li>for neuron `N_j` in output layer `L`, `delta_k^((L)) = ( t_j - o_j^((L)) ) xx gamma_j^((L)) ( v_j^((L)) )`</li>
<li>for neuron `N_j` in hidden layer `l`, `delta_k^((l)) = gamma_j^((l)) ( v_j^((l)) ) xx sum_k delta_k^((l+1)) xx w_(kj)^((l+1))`</li>
</ul>
</p>

<p>
where the summation `sum_k` is over the neurons in layer `(l+1)` and weight `w_(kj)^((l+1))` connects `N_k` in layer `(l+1)` to `N_j` in layer `l`. <br>
</p>

<p>
<strong>Note: </strong>If its not clear, `gamma_j^((l)) ( v_j^((l)) )` is the derivative of the activation function at neuron `N_j`s induced local field. It's not a product.<br>
</p>

<p>
A diagram will help:<br>
</p>

<pre>
Layer l               Layer (l+1)
neuron 1              neuron 1, gradient 1
neuron 2            / neuron 2, gradient 2
...               /   ...
...           w_1j    ...
...           /       ...
...         /         ... 
...       /           ...
neuron j ----w_kj---&gt; neuron k, gradient k
</pre>

<p>
In other words, for a hidden neuron `N_j` in layer `l`, its gradient is the product of (a) the derivate of the activation function at its induced local field and (b) the sum of the product of the gradients in the <em>next </em>layer and the weights that connect `N_j` to that gradient's neuron.<br>
</p>

<p>
<strong>Note:</strong><br>
<ol type="1" start="1">
<li>Its now clear why we needed to store the induced local fields and the values of its impulse functions---calculating these values dynamically significantly reduces the performance of the algorithm.</li>
<li>Don't forget to store the values of the `delta"s"` which will be needed in the next step.</li>
</ol>
</p>

<h2>Constructing the Learning Terms</h2>
<p>
Up to this point, our notation hasn't taken into account the training examples `{bb x(i), bb t(i)}`. We'll do this now by appending a term with `(i)` to denote the value for the ith training example. Ex: `sigma_j^((l))(i)` is the gradient for neuron `N_j` in layer `l` when the network is fed input `bb x(i)` (the ith training example).<br>
</p>

<p>
<strong>Note: </strong>Neuron `N_j` in layer `l` isn't necessarily fed input `bb x(i)`. <em>The network is fed input `bb x(i)` and that in turn completely determines the inputs to neuron `N_j^((l))`.</em><br>
</p>

<p>
This is how we calculate the learning terms:<br>
</p>

<p>
<ul>
<li>Let `{bb x(i), bb t(i)}` be the training examples. `bb x(i)` is a vector `(x_1(i), ..., x_n(i))`.</li>
<li>Let `eta` be the learning parameter.</li>
<li>Let `o_j^((l))(i)` be the output (value of the impulse function) of neuron `N_j^((l))` when the network is fed input `bb x(i)`.</li>
<li>Let weight `w_(kj)^((l))` connect `N_k` in layer `l` to `N_j` in the previous layer. </li>
<li>Let weight `w_(j0)` be the bias for all `j`.</li>
<li>Let `delta_j^((l))(i)` be the gradient of neuron `N_j^((l))` when the network is fed input `bb x(i)`.</li>
</ul>
</p>

<p>
Define `bb y^((l))(i)` as the output from neuron `N_j^((l))` when the network is fed input `bb x(i)`:<br>
</p>

<p>
<ul>
<li>For `l = 0` (input layer), let `y_j^((0))(i) = x_j(i)` (the "output" of the input layer is just the input). </li>
<li>For `l &gt; 0`, let `y_j^((l))(i) = o_j^((l))(i)` (the output of neuron `N_j` in layer `l`).</li>
<li>For all `l`, let `y_0^((l))(i) = +1` (`y_0` gets mapped to the bias).</li>
</ul>
</p>

<p>
<strong>Note:</strong> <br>
<ol type="1" start="1">
<li>`y_j^((l))(i)` can be thought of as the <em>input </em>that's fed to the <em>next </em>layer `(l+1)`.</li>
<li>Since `w_(j0)` is always the bias, the output of neuron `N_j` in layer `l` with weights `bb w` and activation function `phi` is just `\ phi(bb y^((l-1))(i) * bb w) = phi(sum_i y_i^((l-1))(i) w_(ji))` where the sum `sum_i` is over neurons in the previous layer.</li>
</ol>
</p>

<p>
The learning term for weight `w_(kj)` in layer `l` <em>and</em> <em>the ith training example</em> is:<br>
</p>

<p>
`\ Delta w_(kj)^((l))(i) = eta delta_k^((l))(i) y_j^((l-1))(i)`<br>
</p>

<p>
Each weight will have a different learning term for each example<em>.</em> <strong>For each weight `w_(kj)^((l))`, store the sum of all its learning terms for all the training examples, `sum_i Delta w_(kj)^((l))(i)`. </strong>This cumulative sum is what's actually used to adjust the weight. <br>
</p>

<p>
<strong>Note:</strong><br>
<ol type="1" start="1">
<li>Wait until you have the total sum `sum_i Delta w_(kj)^((l))(i)` from all the training examples before adjusting the weights. </li>
<li>Its now clear why we needed to store the values of the impulse functions and the gradients---calculating these values dynamically significantly reduces the performance of the algorithm.</li>
</ol>
</p>

<h2>Adjusting the Weights</h2>
<p>
This is how to adjust the weights:<br>
</p>

<p>
<ul>
<li>Let `w_(kj)^(**(l))` be the <em>previous</em> value of the weight. </li>
<li>If there is no previous value, let `w_(kj)^(**(l)) = 0`.</li>
<li>Let `sum_i Delta w_(kj)^((l))(i)` be the cumulative sum of the learning terms for `w_(kj)^((l))` for all the training examples. </li>
<li>Let `alpha` be the momentum term.</li>
</ul>
</p>

<p>
Adjust the weights using this formula:<br>
</p>

<p>
`\ w_(kj)^((l)) = w_(kj)^((l)) + alpha w_(kj)^(**(l)) + sum_i Delta w_(kj)^((l))(i)`<br>
`\ \ \ \ = w_(kj)^((l)) + alpha w_(kj)^(**(l)) + eta sum_i delta_k^((l))(i) y_j^((l-1))(i)`<br>
</p>

<p>
<strong>Note: </strong>`alpha w_(kj)^(**(l))` is the momentum term.<br>
</p>

<h2>Constructing the Error Function</h2>
<p>
<ul>
<li>Let `{bb x(i), bb t(i)}` be the `p` training examples. </li>
<li>Let `bb o(i)` be the actual output of the network for training input `bb x(i)`. </li>
</ul>
<strong>Note:</strong><br>
<ol type="1" start="1">
<li>Each `bb x(i)` and `bb t(i)` are vectors, `bb x(i) = (x_1(i), ...,  x_n(i))` and `bb t(i) = (t_1(i), ..., t_n(i))`. </li>
<li>Each `t_j(i)` is the expected output of the ith training example for the jth neuron in the output layer.</li>
<li>`bb o(i)` is a vector `o_1(i), ..., o_n(i)`.</li>
<li>`o_j(i)` is the output (value of the impulse function) of neuron `N_j` in the output layer for input `bb x(i)`. </li>
</ol>
</p>

<p>
With these definitions, `bb o(i) - bb t(i)` is the difference in the expected and actual values. We seek to minimize these differences for all training examples:<br>
</p>

<p>
For each training example `i`, construct the error `E_i`<br>
</p>

<p>
`\ E_i = 1/2 ||bb o(i) - bb t(i)||^2`<br>
</p>

<p>
where `||*||` is the <a href="../Appendix/EuclidianDistance.html" title="euclidian distance" class="page">euclidian distance</a>.  Since we're dealing with vectors, each `E_i` actually looks like this:<br>
</p>

<p>
`\ E_i = 1/2 (o_1(i) - t_1(i))^2 + cdots + (o_n(i) - t_n(i))^2`<br>
</p>

<p>
Construct the total error `E` of the network for all training examples:<br>
</p>

<p>
`\ E = sum_i^p E_i = 1/2 sum_i^p ||bb o(i) - bb t(i)||^2 = 1/2 sum_i^p (o_1(i) - t_1(i))^2 + cdots + (o_n(i) - t_n(i))^2`<br>
</p>

<p>
Repeat another iteration of the backpropagation algorithm until the error is less than a pre-selected amount.<br>
</p>

<h1>Troubleshooting</h1>
<p>
<ul>
<li>Unit test, unit test, unit test. You'll be suprised how easy it is to introduce a typo that breaks your implementation.</li>
<li>The error never converges? Make sure you know how the <a href="./Parameters.html" title="backpropagation parameters" class="page">backpropagation parameters</a> work.</li>
</ul>
</p>


			<!-- End wiki content -->
			</div>
			<hr>
			<!-- Backlinks -->
			<div class="footer">
									<b>Backlinks:</b>						<a href='../BackPropagation.html'>NeuralNetworks:BackPropagation</a></li>						<a href='../History.html'>NeuralNetworks:History</a></li>						<a href='./Parameters.html'>NeuralNetworks:BackPropagation:Parameters</a></li>
				<br><br>

				
			<!-- End Backlinks -->
			</div>
		</div>
	</body>
</html>
