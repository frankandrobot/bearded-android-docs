Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2014-07-14T20:41:16-05:00

====== LearningTerm ======
Created Monday 14 July 2014

The **learning term** takes its name because it is the primary term in the backpropagation algorithm that adjusts the weights in the NN. There are actually multiple learning terms --- each weight has its own learning term per example.

===== Notation =====
* Please read the [[Notation|notation page]]

===== Constructing the Learning Terms =====
Given input `bb x[i]`, define `bb y^((l))[i]` as the input that that's fed to the next layer `(l+1)`:

* For `l = 0` (input layer), let `y_j^((0))[i] = x_j[i]` (the "output" of the input layer is just the input). 
* For `l > 0`, let `y_j^((l))[i] = o_j^((l))[i]` (the output of neuron `N_j` in layer `l`).
* For all `l`, let `y_0^((l))[i] = +1` (map `y_0` to the bias).

**Note:**  Since `w_(j0)` is always the bias, we can write the output of neuron `N_j^((l+1))` in terms of `y^((l))`:

`\ o_j^((l+1))[i] = phi(bb y^((l))[i] * bb w) = phi(sum_k y_k^((l))[i] w_(jk))`

where the sum `sum_k` is over the neurons in the previous layer.

The **learning term** for weight `w_(kj)` in layer `l` //and the ith training example// is:

`\ Delta w_(kj)^((l))[i] = eta delta_k^((l))[i] y_j^((l-1))[i]`

As is clear from the formula, each weight has a different learning term for each example.
