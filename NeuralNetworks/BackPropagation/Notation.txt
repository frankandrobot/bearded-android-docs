Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2014-07-14T21:39:27-05:00

====== Notation ======
Created Monday 14 July 2014

* `L` = the output layer.
* `(bb x, bb t)` = [[TrainingExamples|training example]]
* `N_j^((l))` = neuron `j` in layer `l`
* `t_j` = the expected output of `N_j^((L))` //when the network input is `bb x`//
* `o_j^((l))` = the actual output of `N_j^((l))` = impulse function value of `N_j^((l))` //when the network input is `bb x`//
* `v_j^((l))` = the induced local field of `N_j^((l))` //when the network input is `bb x`//
* `gamma_j^((l))` = the derivative of `N_j^((l))`s [[NeuralNetworks:ActivationFunction|activation function]] `phi`.
* In general, let superscript `l` denote layer `l`.
* Let weight `w_(kj)^((l+1))` connect `N_k^((l+1))`  to `N_j^((l))`

'''
Layer l               Layer (l+1)
neuron 1              neuron 1
...                   ...
...                   ...
neuron j ----w_kj---> neuron k
...                   ...	
'''
* Let `w_(k0)^((l)` be the [[NeuralNetworks:Neuron|bias]]

**Note 1:** Neuron `N_j^((l))` isn't necessarily fed input `bb x`. //The network is fed input `bb x` and that in turn completely determines the inputs to neuron `N_j^((l))`.//
**Note 2:** `t_j` is the expected //network output//, that is, the output of the neuron //j// in the //output layer.//
**Note 3:** Recall that there are two outputs associated with a [[NeuralNetworks:Neuron|neuron]] at input `bb y` --- the //induced local field// `bb w * bb y` and the //value of its impulse function// `phi(bb w * bb y)`. 
**Note 4:** `gamma_k` is a function. `v_j` and `o_j` are real numbers.
**Note 5:** since `w_(k0)^((l))` is the bias, it's not really connected to a neuron in the previous layer.

===== More Than One Training Example =====
Up to this point, the notation doesn't take into account multiple training examples. To associate a term with the ith training example `{bb x[i], bb t[i]}`, use square brackets. For example,

* Let `o_j^((l))[i]` be the output (value of the impulse function) of neuron `N_j^((l))` when the network is fed input `bb x[i]`.
* Let `delta_j^((l))[i]` be the gradient of neuron `N_j^((l))` when the network is fed input `bb x[i]`.
