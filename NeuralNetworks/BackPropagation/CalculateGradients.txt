Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2014-05-01T23:16:39-04:00

====== CalculateGradients ======
Created Thursday 01 May 2014

===== Notation =====
* `L` = the output layer.
* `(bb x, bb t)` = [[TrainingExamples|training example]]
* `N_j^((l))` = neuron `j` in layer `l`
* `t_j` = the expected output of `N_j^((L))` //when the network input is `bb x`//
* `o_j^((l))` = the actual output of `N_j^((l))` = impulse function value of `N_j^((l))` //when the network input is `bb x`//
* `v_j^((l))` = the induced local field of `N_j^((l))` //when the network input is `bb x`// 
* `gamma_j^((l))` = the derivative of `N_j^((l))`s [[NeuralNetworks:ActivationFunction|activation function]].
* (In general, let a superscript `l` denote layer `l`.)
* Let weight `w_(kj)^((l+1))` connect `N_k^((l+1))`  to `N_j^((l))`

'''
Layer l               Layer (l+1)
neuron 1              neuron 1
...                   ...
...                   ...
neuron j ----w_kj---> neuron k
...                   ...	
'''

**Note 1: **`gamma_k` is a function. `v_j` and `o_j` are real numbers.
**Note 2: **`t_j` is the expected //network output//, that is, the output of the neuron //j //in the //output layer.//

===== Calculating the Gradients `delta"s"` =====
Define the local [[NeuralNetworks:Appendix:Gradient|gradient]] `delta_j^((l))` of neuron `j` recursively as follows:

* for neuron `N_j^((L))` (output layer), `delta_j^((L)) = ( t_j - o_j^((L)) ) xx gamma_j^((L)) ( v_j^((L)) )`.
* for neuron `N_j^((l))` (hidden layer), `delta_j^((l)) = gamma_j^((l)) ( v_j^((l)) ) xx sum_k delta_k^((l+1)) xx w_(kj)^((l+1))` where the summation `sum_k` is over the neurons in layer `(l+1)`. 

In other words, for a hidden neuron `N_j^((l))`, its gradient is the product of (a) the derivate of the activation function at its induced local field and (b) the sum of the product of the gradients in the //next //layer and the weights that connect `N_j` to that gradient's neuron.

'''
Layer l               Layer l+1
neuron 1              neuron 1, gradient 1
neuron 2            / neuron 2, gradient 2
...               /   ...
...           w_1j    ...
...           /       ...
...         /         ... 
...       /           ...
neuron j ----w_kj---> neuron k, gradient k
'''

**Note 1: **`gamma_j^((l)) ( v_j^((l)) )` = the derivative of the activation function at neuron `N_j`s induced local field `!=` a product.
**Note 2: **Its now clear why we needed to store the induced local fields and the values of its impulse functions---calculating these values dynamically significantly reduces the performance of the algorithm.
