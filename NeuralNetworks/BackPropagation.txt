Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2013-11-18T11:20:27-05:00

====== BackPropagation ======
Created Monday 18 November 2013

You may want to first read the [[+Motivation|motivation]] for the back propagation algorithm.

===== The Backpropagation Algorithm =====
Step 1. Initialize the weights.
Step 2. Construct the error function.
Step 3. Perform the backpropagation.
Step 4. Adjust the weights.
Step 5. Repeat steps 2--4 until the error is smaller than a pre-selected value.

===== Initalizing the Weights =====
"Pick the weights from a uniform distribution whose mean is zero and whose variance is chosen to make the standard deviation of the induced local fields of the neurons (See [[:NeuralNetworks]] ) lie at //the transition between the linear and saturated parts of the +ActivationFunction// " (I have no idea what the emphasized parts means.)

Most researchers initialize the weights to random values between 0 and 1.

===== Constructing the Error Function =====
Calculate the output of the network for each training example. Use the output to compute `E_i`.

Gotcha 1: recall that we have to append a 1 to each input because of the bias. Make sure to append a 1 to the output from a previous layer to the next layer.

Gotcha 2: for each neuron, make sure to save the value of its induced local field and impulse function. You'll need these values for the backpropagation. 

===== Performing the Backpropagation =====

For each training example, compute the local gradients (`theta"s"`) of each neuron. 
Let L be the output layer. 
Let `v_j^((l))` be the induced local field of neuron `j` in layer `l`. 
Let `o_j^((l))` be the output (value of the impulse function) of neuron `j` in layer `l`.
Let `gamma_j^((l))` be the derivative of neuron `j`s [[ActivationFunction|activation function]].
(In general, a superscript `l` corresponds to layer `l`.)

**Note: **`gamma_k` is a function. `v_j` and `o_j` are real numbers.

Define the local gradient `theta_j^((l))` of neuron `j` recursively as follows:

* for neuron `j` in output layer `L`, `theta_k^((L)) = ( t_j - o_j^((L)) ) xx gamma_j^((L)) ( v_j^((L)) )`
* for neuron `j` in hidden layer `l`, `theta_k^((l)) = gamma_j^((l)) ( v_j^((l)) ) xx sum_k theta_k^((l+1)) xx w_(kj)^((l+1))`

where weight `w_kj^((l+1))` connects neuron `j` in layer `l` to neuron k in layer `(l+1)`. A diagram will help:

'''
Layer l               Layer (l+1)
neuron 1              neuron 1, gradient 1
...                 / ...
...               /   ...
...           w_1j    ...
...           /       ...
...         /         ... 
...       /           ...
neuron j ----w_kj---> neuron k, gradient k
'''


In other words, for a hidden neuron `j` in layer `l`, take the sum of the product of the gradients in the //next //layer and the weights that connect neuron `j` to the neuron of that local gradient.

**Note: **If its not clear, `gamma_j^((l)) ( v_j^((l)) )` is the value of the derivative of the activation function at the induced local field. It's not multiplication.

Since the recursion starts in the output layer, the computations proceed from the back to the front (hence the name "backpropagation").

===== Adjusting the Weights =====
Use the local gradients to adjust the weights. 

`w_kj^l = w_kj^l + eta sigma^l_j`





