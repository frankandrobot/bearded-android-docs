Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2013-11-18T11:20:27-05:00

====== BackPropagation ======
Created Monday 18 November 2013

You may want to first read the [[+Motivation|motivation]] for the back propagation algorithm.

===== The Backpropagation Algorithm (Version A) =====
Step 1. Initialize the weights.
Step 2. Construct the error function.
Step 3. Perform the backpropagation.
Step 4. Adjust the weights.
Step 5. Repeat steps 2--4 until the error is smaller than a pre-selected value.

Pros: if F is the neural network function and `{(bb x_i, bb t_i)}` is the training data, then `F(bb x_i) = bb t_i`.
Cons: Uses a heck of a lot of memory. For mobile, this is probably not the way to go.

===== Initalizing the Weights =====
"Pick the weights from a uniform distribution whose mean is zero and whose variance is chosen to make the standard deviation of the induced local fields of the neurons (See [[:NeuralNetworks]] ) lie at //the transition between the linear and saturated parts of the +ActivationFunction// " (I have no idea what the emphasized parts means.)

Most researchers initialize the weights to random values between 0 and 1.

===== Constructing the Error Function =====
For each training example:
* use the output to compute `E_i`.
* store the values of induced local fields and impulse functions. You'll need these values for the backpropagation. 

**Gotcha 1:** recall that we have to append a 1 to each input because of the bias. Make sure to append a 1 to the output from a previous layer to the next layer.

**Gotcha 2:** for each training example and each neuron, you save value of its induced local field and impulse function. This can be prohibitive depending on the neural network.

===== Performing the Backpropagation =====

For each training example:

* compute the local gradients (`delta"s"`) of each neuron. 

Let L be the output layer. 
Let `v_j^((l))` be the induced local field of neuron `j` in layer `l`. 
Let `o_j^((l))` be the output (value of the impulse function) of neuron `j` in layer `l`.
Let `gamma_j^((l))` be the derivative of neuron `j`s [[ActivationFunction|activation function]].
(In general, a superscript `l` corresponds to layer `l`.)

**Note: **`gamma_k` is a function. `v_j` and `o_j` are real numbers.

Define the local gradient `delta_j^((l))` of neuron `j` recursively as follows:

* for neuron `j` in output layer `L`, `delta_k^((L)) = ( t_j - o_j^((L)) ) xx gamma_j^((L)) ( v_j^((L)) )`
* for neuron `j` in hidden layer `l`, `delta_k^((l)) = gamma_j^((l)) ( v_j^((l)) ) xx sum_k delta_k^((l+1)) xx w_(kj)^((l+1))`

where weight `w_kj^((l+1))` connects neuron `j` in layer `l` to neuron k in layer `(l+1)`. A diagram will help:

'''
Layer l               Layer (l+1)
neuron 1              neuron 1, gradient 1
...                 / ...
...               /   ...
...           w_1j    ...
...           /       ...
...         /         ... 
...       /           ...
neuron j ----w_kj---> neuron k, gradient k
'''

In other words, for a hidden neuron `j` in layer `l`, take the sum of the product of the gradients in the //next //layer and the weights that connect neuron `j` to the neuron of that local gradient.

**Note: **If its not clear, `gamma_j^((l)) ( v_j^((l)) )` is the value of the derivative of the activation function at the induced local field. It's not multiplication.

Since the recursion starts in the output layer, the computations proceed from the back to the front (hence the name "backpropagation").

* Make sure to store the values of the `delta"s"`

===== Adjusting the Weights =====
Use the local gradients to adjust the weights. 

`w_(kj)^((l)) = w_(kj)^((l)) + alpha(w_(kj)^(**(l))) + eta sigma_j^((l)) o_k^((l-1))`

where `w_(kj)^(**(1))` is the value of the weight in the //previous// time step. If there is no previous value, then this term defaults to 0.

* Make sure to store the previous values of the weights

**Note: **Make sure to do this step //after //computing the gradients. Otherwise, it won't work.





