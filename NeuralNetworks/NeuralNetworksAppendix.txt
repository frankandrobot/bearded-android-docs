Content-Type: text/x-zim-wiki
Wiki-Format: zim 0.4
Creation-Date: 2013-11-15T10:31:42-05:00

====== NeuralNetworksAppendix ======
Created Friday 15 November 2013

===== Required Math Concepts =====
[[HessianMatrix]]

Gradient is defined as an operator \( \nabla \) (pronounced del operator)

$$ \nabla f(x,y) = f_x \hat i + f_y \hat j$$

where \(f_x\) is the partial derivative of f with respect to x.

* In 2D, it points in the direction of the biggest slope.
* In 1D, the gradient reduces to \( \nabla f(x) = f_x \hat i\). In other words, in 1D, the gradient of a function is just the slope. 
