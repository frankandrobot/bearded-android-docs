<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Bearded-android-docs : Android quick docs" />

    <link rel="stylesheet" type="text/css" media="screen" href="
http://frankandrobot.github.io/bearded-android-docs/stylesheets/stylesheet.css">

    <title>History</title>
    <meta name='Generator' content='Zim 0.60'>
    
    <script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/2.3-latest/MathJax.js?config=AM_HTMLorMML.js">
</script>

  </head>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/frankandrobot/bearded-android-docs">View on GitHub</a>

          <h1 id="project_title">Bearded-android-docs</h1>
          <h2 id="project_tagline">History</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/frankandrobot/bearded-android-docs/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/frankandrobot/bearded-android-docs/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">

<!-- Wiki content -->

<!-- -->
<!-- <h1>History</h1> -->
<!-- -->

<p>
Created Tuesday 31 December 2013<br>
</p>

<p>
Two neural networks are at a bar. One says to the  other, "Hey, I learned that if A then B". The other replies, "Wow, I know that if B then C". "Why don't we teach ourselves if A then C?" A while later, they ask each other "What was it what we were talking about?"<br>
</p>

<p>
The joke is in reference to the fact that while neural networks can be used to create powerful technologies (self-driving car, voice recognition like Google Now, etc), neural networks are actually pretty stupid. The reality is that machine learning hasn't advanced much when it comes to creating a machine that can actually think for itself i.e., "sentient" artificial intelligence. <br>
</p>

<p>
As per <a href="http://www.ted.com/talks/jeff_hawkins_on_how_brain_science_will_change_computing.html" title="this TED talk" class="http">this TED talk</a>, the real problem is that there doesn't seem to be a common "theory of learning."<br>
</p>

<h2>Why Neural Networks?</h2>

<p>
<a href="../NeuralNetworks.html" title="Neural networks" class="page">Neural networks</a> (NNs) are easy to understand conceptually and (somewhat) easy to code. A strong math background is also required.<br>
</p>

<p>
NNs are good at these types of problems:<br>
</p>

<p>
<ul>
<li><strong>Classification</strong> - the goal is to assign the input object to a predetermined class or group. We provide input objects that are representative of all groups (training examples). NN deduces classification rules from training examples. Ex: handwriting recognition </li>
<li><strong>Prediction</strong> - ex: sun cycles</li>
<li><strong>Clustering (data mining)</strong> - Similar to classification. However, we don't provide the representative groups. The goal is to figure out the groups that partition the training example. Ex: learn characters in an alien language from sample writings. (This is a type of <em>unsupervised learning</em>.)</li>
<li><strong>Pattern Association</strong> - pick out faces from blurry photographs</li>
<li><strong>Optimization</strong> - minimizing or maximizing a function</li>
</ul>
</p>

<p>
In general, NNs are good at <em>bottom-up learning</em>. In contrast, to top-down learning, in bottom-up learning, hard and fast rules either don't exist or are too complicated to express. Real world example: the rules managers use to distribute work at a customer service center. (Managers may not be able to completely verbalize what they do.) <br>
</p>

<h2>A Brief History</h2>

<p>
<ul>
<li>Neurons were researched over 100 years ago. Scientists discovered that our brain is made up of millions of connected neurons.</li>
<li>first mathematical model of a neuron was created in 1943 by Warren McCulloch and Walter Pitts </li>
<li>The original neuron model is basically a linear function. Each coordinate is multiplied by a weight and the sum of each product is the output of the neuron.</li>
<li>Neurons working together in parallel is a <em>single layer of a neural network</em>. Layers can be stacked together to create <em>multi-layer neural networks</em>. </li>
<li>By the 60s, NNs had a lot of hype---robotic servants, etc.</li>
<li>A book in 1969 by Minsky helped destroy interest in neural networks for the next decade. </li>
<li>Minksy pointed out the elephant in the room. Single layer neural networks can model only linear data. </li>
<li>Research decreased during the 70s because: (1) computers weren't (yet) powerful enough for research and (2) no motivation or $money$ (thanks to Minksy)</li>
<li>It wasn't until the 80's that interest in NNs surged again. In particular, the <a href="./BackPropagation.html" title="backpropagation algorithm" class="page">backpropagation algorithm</a> was popularized by Rumelhart. (Up until this point, there was no obvious way to automatically train an NN with sample data.)</li>
</ul>
</p>

<h2>Different Types of NNs</h2>

<p>
There are actually different types of NNs suitable for different types of problems:<br>
</p>

<p>
<ul>
<li><strong>recursive neural networks</strong> - sentiment analysis (natural language processing)</li>
<li><a href="./ConvolutionalNetwork.html" title="convolution networks" class="page">convolution networks</a> - image processing</li>
<li>many more</li>
</ul>
</p>

<h2>The Backpropagation Algorithm</h2>

<p>
The backpropagation algorithm is a popular way of training neural networks. The basic form is described <a href="./BackPropagation/BatchModeBackPropagation.html" title="here" class="page">here</a>.  <br>
</p>

<p>
<ul>
<li>The backpropagation algorithm is a form of <em>supervised learning</em> because you supply the examples.</li>
<li>Its called "backprop" because it starts from the output layer and recursively works backwards to the input layer.</li>
<li>We make one small change to the neuron model in order for the backprop algorithm to be guaranteed to work---apply an <a href="./ActivationFunction.html" title="activation function" class="page">activation function</a> to the output of the neuron. (A commonly used one is <code>tanh</code>. Activation functions have certain math properties that allow the math to work.)</li>
<li>The algorithm reduces the error one small step at a time. It iteratively travels along the direction of highest slope until error is less than given value.</li>
<li>The problem is that we don't know if we reached a local minimum or a global minimum. Theoretically, the algorithm can get stuck even when you add a correction term. However, it doesn't in practice and scientists aren't really sure why.</li>
</ul>
</p>

<h2>Some Resources</h2>

<p>
<ul>
<li><a href="http://jeremykun.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/" title="http://jeremykun.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/" class="http">http://jeremykun.com/2011/08/11/the-perceptron-and-all-the-things-it-cant-perceive/</a></li>
<li><a href="http://www.cogsci.rpi.edu/~rsun/sun-zhang-jcsr2004-f.pdf" title="http://www.cogsci.rpi.edu/~rsun/sun-zhang-jcsr2004-f.pdf" class="http">http://www.cogsci.rpi.edu/~rsun/sun-zhang-jcsr2004-f.pdf</a></li>
<li><a href="http://www-cs-faculty.stanford.edu/~eroberts/courses/soco/projects/neural-networks/History/history1.html" title="http://www-cs-faculty.stanford.edu/~eroberts/courses/soco/projects/neural-networks/History/history1.html" class="http">http://www-cs-faculty.stanford.edu/~eroberts/courses/soco/projects/neural-networks/History/history1.html</a></li>
<li><em>Elements of Artificial Neural Networks</em>. Kishan Mehrotra, Chilukuri K. Mohan and Sanjay Ranka</li>
</ul>
</p>


<!-- End wiki content -->

<hr />

<!-- Attachments and Backlinks -->

	<i>No backlinks to this page.</i>



<!-- End Attachments and Backlinks -->

    <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'beardedandroid'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
    
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Bearded-android-docs maintained by <a href="https://github.com/frankandrobot">frankandrobot</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
        <p><a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-sa/4.0/80x15.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.</p>

      </footer>
    </div>

    

  </body>
</html>
